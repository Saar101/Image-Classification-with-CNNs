{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7cf3460",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 1: Import Libraries & Initialize Data Importer\n",
    "Import required packages and define the `DataImporter` class for automatic Kaggle dataset retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3c5b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "class DataImporter:\n",
    "    def __init__(self):\n",
    "        self.DATASET_SLUG = \"drgfreeman/rockpaperscissors\"\n",
    "        self.DATASET_URL = f\"https://www.kaggle.com/datasets/{self.DATASET_SLUG}\"\n",
    "        self.DATA_DIR = \"./data\"\n",
    "\n",
    "    def import_data(self):\n",
    "        print(\"Dataset URL:\", self.DATASET_URL)\n",
    "        print(\"DATA_DIR:\", self.DATA_DIR)\n",
    "\n",
    "        os.makedirs(self.DATA_DIR, exist_ok=True)\n",
    "\n",
    "        def find_dataset_folder():\n",
    "            for root, dirs, files in os.walk(self.DATA_DIR):\n",
    "                dir_names = set(dirs)\n",
    "                if {\"rock\", \"paper\", \"scissors\"}.issubset(dir_names):\n",
    "                    return root\n",
    "            return None\n",
    "\n",
    "        dataset_folder = find_dataset_folder()\n",
    "\n",
    "        if dataset_folder is None:\n",
    "            print(\"Dataset not found locally. Downloading from Kaggle...\")\n",
    "\n",
    "            api = KaggleApi()\n",
    "            api.authenticate()\n",
    "\n",
    "            api.dataset_download_files(\n",
    "                self.DATASET_SLUG,\n",
    "                path=self.DATA_DIR,\n",
    "                unzip=True\n",
    "            )\n",
    "\n",
    "            print(\"Download and extract completed.\")\n",
    "            dataset_folder = find_dataset_folder()\n",
    "            if dataset_folder is None:\n",
    "                raise RuntimeError(\n",
    "                    f\"Download finished, but could not find 'rock','paper','scissors' under {self.DATA_DIR}\"\n",
    "                )\n",
    "        else:\n",
    "            print(\"Dataset already exists locally:\", dataset_folder)\n",
    "\n",
    "        print(\"USING DATASET FOLDER:\", dataset_folder)\n",
    "        return dataset_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84588d0",
   "metadata": {},
   "source": [
    "## ðŸ”½ Step 2: Download & Extract Dataset from Kaggle\n",
    "Automatically download the rock-paper-scissors dataset (if not already present locally) and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c779c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/drgfreeman/rockpaperscissors\n",
      "DATA_DIR: ./data\n",
      "Dataset already exists locally: ./data\n",
      "USING DATASET FOLDER: ./data\n",
      "âœ“ Dataset ready at: ./data\n"
     ]
    }
   ],
   "source": [
    "importer = DataImporter()\n",
    "data_dir = importer.import_data()\n",
    "print(\"âœ“ Dataset ready at:\", data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd81884",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 3: Load & Prepare Data Pipeline\n",
    "Create TensorFlow datasets with batching, shuffling, and normalization. Verify image shapes and class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8833109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied rock to temp directory\n",
      "Copied paper to temp directory\n",
      "Copied paper to temp directory\n",
      "Copied scissors to temp directory\n",
      "Found 2188 files belonging to 3 classes.\n",
      "Using 1751 files for training.\n",
      "Copied scissors to temp directory\n",
      "Found 2188 files belonging to 3 classes.\n",
      "Using 1751 files for training.\n",
      "Found 2188 files belonging to 3 classes.\n",
      "Using 437 files for validation.\n",
      "Found 2188 files belonging to 3 classes.\n",
      "Using 437 files for validation.\n",
      "Classes: ['paper', 'rock', 'scissors']\n",
      "Classes: ['paper', 'rock', 'scissors']\n",
      "Batch shape: (32, 150, 150, 3) Labels shape: (32,)\n",
      "Batch shape: (32, 150, 150, 3) Labels shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.utils import image_dataset_from_directory\n",
    "    import shutil\n",
    "    import tempfile\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print('Data directory not found at', data_dir)\n",
    "    else:\n",
    "    \n",
    "        # Filter out unwanted directories (like 'rps-cv-images')\n",
    "        valid_classes = ['rock', 'paper', 'scissors']\n",
    "        \n",
    "        # Create a temporary directory with only the valid classes\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        for class_name in valid_classes:\n",
    "            src_dir = os.path.join(data_dir, class_name)\n",
    "            dst_dir = os.path.join(temp_dir, class_name)\n",
    "            if os.path.exists(src_dir):\n",
    "                shutil.copytree(src_dir, dst_dir)\n",
    "                print(f'Copied {class_name} to temp directory')\n",
    "        \n",
    "        # Use the temporary directory with only valid classes\n",
    "        train_ds = image_dataset_from_directory(temp_dir, validation_split=0.2, subset='training', seed=123, image_size=(150,150), batch_size=32) \n",
    "        val_ds = image_dataset_from_directory(temp_dir, validation_split=0.2, subset='validation', seed=123, image_size=(150,150), batch_size=32) \n",
    "        class_names = train_ds.class_names\n",
    "        print('Classes:', class_names)\n",
    "        for images, labels in train_ds.take(1):\n",
    "            print('Batch shape:', images.shape, 'Labels shape:', labels.shape)\n",
    "        \n",
    "        # Clean up temporary directory after loading\n",
    "        shutil.rmtree(temp_dir)\n",
    "except Exception as e:\n",
    "    print('Could not create tf datasets:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c645b",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 4: Build, Train & Evaluate CNN Model\n",
    "Construct a 3-layer CNN, compile with Adam optimizer, train with early stopping, and analyze predictions & misclassified examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c87ca186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2188 files belonging to 3 classes.\n",
      "Using 1751 files for training.\n",
      "Using 1751 files for training.\n",
      "Found 2188 files belonging to 3 classes.\n",
      "Using 437 files for validation.\n",
      "Found 2188 files belonging to 3 classes.\n",
      "Using 437 files for validation.\n",
      "Classes: ['paper', 'rock', 'scissors']\n",
      "Number of classes: 3\n",
      "Classes: ['paper', 'rock', 'scissors']\n",
      "Number of classes: 3\n",
      "Batch shape: (32, 150, 150, 3) Labels shape: (32,)\n",
      "Batch shape: (32, 150, 150, 3) Labels shape: (32,)\n",
      "Training config: RUN_FULL_TRAIN=True, EARLY_STOPPING_PATIENCE=5, NUM_EPOCHS=30\n",
      "Callbacks: ['ModelCheckpoint']\n",
      "Epoch 1/30\n",
      "Training config: RUN_FULL_TRAIN=True, EARLY_STOPPING_PATIENCE=5, NUM_EPOCHS=30\n",
      "Callbacks: ['ModelCheckpoint']\n",
      "Epoch 1/30\n",
      "\u001b[1m18/55\u001b[0m \u001b[32mâ”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m21s\u001b[0m 571ms/step - accuracy: 0.3800 - loss: 1.1351"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining config: RUN_FULL_TRAIN=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRUN_FULL_TRAIN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, EARLY_STOPPING_PATIENCE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEARLY_STOPPING_PATIENCE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, NUM_EPOCHS=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mCallbacks:\u001b[39m\u001b[33m'\u001b[39m, [\u001b[38;5;28mtype\u001b[39m(cb).\u001b[34m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks])\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m model.save(\u001b[33m'\u001b[39m\u001b[33mrps_cnn_final.keras\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     93\u001b[39m test_loss, test_acc = model.evaluate(val_ds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nrsaa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nrsaa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nrsaa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nrsaa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nrsaa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nrsaa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nrsaa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nrsaa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nrsaa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nrsaa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nrsaa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nrsaa\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "RUN_FULL_TRAIN = True \n",
    "EARLY_STOPPING_PATIENCE = 5 \n",
    "NUM_EPOCHS = 30  \n",
    "try:  \n",
    "    import tensorflow as tf \n",
    "    from tensorflow.keras.utils import image_dataset_from_directory  \n",
    "    import shutil\n",
    "    import tempfile\n",
    "    \n",
    "    if not os.path.exists(data_dir): \n",
    "        print('Data directory not found at', data_dir) \n",
    "    else:\n",
    "        # Filter out unwanted directories (like 'rps-cv-images')\n",
    "        valid_classes = ['rock', 'paper', 'scissors']\n",
    "        \n",
    "        # Create a temporary directory with only the valid classes\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        for class_name in valid_classes:\n",
    "            src_dir = os.path.join(data_dir, class_name)\n",
    "            dst_dir = os.path.join(temp_dir, class_name)\n",
    "            if os.path.exists(src_dir):\n",
    "                shutil.copytree(src_dir, dst_dir)\n",
    "        \n",
    "        # Use the temporary directory with only valid classes\n",
    "        train_ds = image_dataset_from_directory(\n",
    "            temp_dir,\n",
    "            validation_split=0.2,\n",
    "            subset='training',\n",
    "            seed=123,\n",
    "            image_size=(150, 150),\n",
    "            batch_size=32,\n",
    "        ) \n",
    "        val_ds = image_dataset_from_directory(\n",
    "            temp_dir,\n",
    "            validation_split=0.2,\n",
    "            subset='validation',\n",
    "            seed=123,\n",
    "            image_size=(150, 150),\n",
    "            batch_size=32,\n",
    "        ) \n",
    "        class_names = train_ds.class_names \n",
    "        print('Classes:', class_names) \n",
    "        num_classes = len(class_names)\n",
    "        print('Number of classes:', num_classes)\n",
    "        for images, labels in train_ds.take(1): \n",
    "            print('Batch shape:', images.shape, 'Labels shape:', labels.shape) \n",
    "       \n",
    "        try:\n",
    "            \n",
    "            AUTOTUNE = tf.data.AUTOTUNE\n",
    "            train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "            val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "            \n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Rescaling(1./255, input_shape=(150,150,3)),  \n",
    "                tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "                tf.keras.layers.MaxPooling2D((2,2)),\n",
    "                tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "                tf.keras.layers.MaxPooling2D((2,2)),\n",
    "                tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "                tf.keras.layers.MaxPooling2D((2,2)),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(128, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "                tf.keras.layers.Dense(num_classes, activation='softmax') \n",
    "            ])\n",
    "\n",
    "           \n",
    "            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "           \n",
    "            callbacks = [\n",
    "                tf.keras.callbacks.ModelCheckpoint('best_rps_model.keras', save_best_only=True, monitor='val_loss'),\n",
    "            ]\n",
    "            if not RUN_FULL_TRAIN:\n",
    "                callbacks.append(\n",
    "                    tf.keras.callbacks.EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        patience=EARLY_STOPPING_PATIENCE,\n",
    "                        restore_best_weights=True\n",
    "                    )\n",
    "                )\n",
    "          \n",
    "            print(f\"Training config: RUN_FULL_TRAIN={RUN_FULL_TRAIN}, EARLY_STOPPING_PATIENCE={EARLY_STOPPING_PATIENCE}, NUM_EPOCHS={NUM_EPOCHS}\")\n",
    "         \n",
    "            print('Callbacks:', [type(cb).__name__ for cb in callbacks])\n",
    "\n",
    "            history = model.fit(train_ds, validation_data=val_ds, epochs=NUM_EPOCHS, callbacks=callbacks)\n",
    "\n",
    "            model.save('rps_cnn_final.keras')\n",
    "\n",
    "            test_loss, test_acc = model.evaluate(val_ds)\n",
    "            print('Final test accuracy:', test_acc)\n",
    "\n",
    "            import numpy as np\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            val_images = np.concatenate([x.numpy() for x, y in val_ds], axis=0)\n",
    "            val_labels = np.concatenate([y.numpy() for x, y in val_ds], axis=0)\n",
    "\n",
    "            preds = model.predict(val_images)\n",
    "            pred_labels = np.argmax(preds, axis=1)\n",
    "\n",
    "            mis_idx = np.where(pred_labels != val_labels)[0]\n",
    "            print('Number of misclassified examples in validation set:', len(mis_idx))\n",
    "\n",
    "            n_show = min(6, len(mis_idx))\n",
    "            for i in range(n_show):\n",
    "                idx = mis_idx[i]\n",
    "                plt.figure(figsize=(3,3))\n",
    "                plt.imshow(val_images[idx].astype('uint8'))\n",
    "                plt.title(f\"True: {class_names[pred_labels[idx]]}  Pred: {class_names[val_labels[idx]]}\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "        except Exception as e:\n",
    "            print('Model build/train/eval skipped or failed:', e)\n",
    "        finally:\n",
    "            # Clean up temporary directory\n",
    "            if 'temp_dir' in locals() and os.path.exists(temp_dir):\n",
    "                shutil.rmtree(temp_dir)\n",
    "except Exception as e:\n",
    "    print('Could not create tf datasets:', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
